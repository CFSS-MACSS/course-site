---
title: "Text analysis: fundamentals"
author: "MACSS 30500 <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "24px",
  code_font_size = "20px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
xaringanExtra::use_panelset()
```

```{r pkgs, include = FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(tidytext)
library(scales)
library(here)
library(patchwork)
library(magrittr)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

class: inverse, middle

# Regular Expressions


---

### What are regular expressions? Why are they for?

We use them to manipulate character data, aka strings. 

Regular Expressions or regexes (singular regex): **language for pattern matching**. They are strings containing normal characters and special meta-characters that describe a particular pattern that we want to match in a given text.

Regular Expressions are used:

* in **many programming languages**
* for **any task that deals with text:** NLP or data-cleaning tasks (e.g., find words that include a given set of letters, how often do past tenses occur in a text, find emails or phone numbers, find and replace left over HTML tags from scraping, etc.).

<!--
Given our ability to manipulate strings and our ability to test for equivalence (==) or test whether some string contains another (in), we don't technically need special functions for pattern matching (e.g. regular expressions). That said, it becomes very tedious very quickly if we have to write all our pattern-matching code ourselves
-->

---

### Regex examples

Examples: download today's in-class materials from the website

Resources:
* [stringr cheat sheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf) for a complete overview of all `stringr` functions
* [Chapter 14 "Strings" of R for Data Science](https://r4ds.had.co.nz/strings.html#strings), especially section 14.4 "Tools" for examples of each of these functions
* [Excellent tutorial](https://github.com/ziishaned/learn-regex/blob/master/README.md)

---

### The `stringr()` package in R

When you use regular expressions for your analysis, most likely you will need to use your regular expression together with one of the functions from the `stringr()` package. 

This package includes several functions that let you: detect matches in a string, count the number of matches, extract them. replace them with other values, or split a string based on a match. 

---

### The `stringr()` package in R

Fundamental `stringr()` functions:

`str_detect()`: detect matches in a string
`str_count()`: count the number of matches
`str_extract()` and `str_extract_all()`: extract matches
`str_replace()` and `str_replace_all()`: replace matches
`str_split()`: split a string based on a match

Key resources:
* [Chapter 14 "Strings" of R for Data Science](https://r4ds.had.co.nz/strings.html#strings), especially section 14 for examples of each of these functions
* [Cheat sheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

---

class: inverse, middle

# Basic workflow for text analysis

---

## Basic workflow for text analysis

We can think at the basic workflow as a 4-step process:

1. Obtain your textual data

1. Data cleaning and pre-processing 

1. Data transformation

1. Perform analysis

Let's review each step...

---

## 1. Obtain your textual data

**Common data sources for text analysis:**

* Online (Scraping and/or APIs)
* Databases
* PDF documents
* Digital scans of printed materials

---

## 1. Obtain your textual data

**Corpus and document:**

* Textual data are usually referred to as **corpus**: general term to refer to a collection of texts, stored as raw strings (e.g., a set of articles from the NYT, novels by an author, one or multiple books, etc.)

* Each corpus might have separate articles, chapters, pages, or even paragraphs. Each individual unit is called a **document**. You decide what constitutes a document in your corpus. 

---

## 2. Data cleaning and pre-processing

**Standard cleaning and pre-processing tasks:**

* Tokenize the text (to n-grams)
* Convert to lower case
* Remove punctuation and numbers
* Remove stopwords (standard and custom/domain-specific)
* Remove or replace other unwanted tokens
* Stemming or Lemmatization 

---

## 2. Data cleaning and pre-processing

**Tokenize the text (to n-grams) mean splitting your text into single tokens**. 

**Token:** word, alphanumeric character, !, ?, number, emoticon etc.

Most tokenizers split on white spaces, but they need also consider exceptions such as contractions (I'll, dog's), hyphens in or between words (e-mail, co-operate, take-it-or-leave, 30-year-old).

**N-gram:** a contiguous sequence of n items from a given text (items can be syllables, letters, words, etc.). We usually keep unigrams (the single word), but there instances in which bigrams are helpful: for example "Joe Biden" is a bi-gram.  
---

## 2. Data cleaning and pre-processing

**Remove stopwords** (standard and custom/domain-specific) 
* Examples: the, is, are, a, an, in, etc.
* Why we want to remove them? 
* Example: if you are working on a corpus that talks about "President Biden" you might want to add "Biden" among your stop words

---

## 2. Data cleaning and pre-processing

**Stemming** and **lemmatization** are similar in that both aim at simplifying words (aka tokens) to their base form but they do it differently. Why do we want to do it?

--

**Stemming:** reducing a token to its **root stem** by brutally removing parts from them 
* Examples: dogs become dog, walked becomes walk
* Faster, but not always accurate. Example: caring becomes car, changing becomes chang, better becomes bett

**Lemmatization:** reducing a token to its **root lemma** by using their meaning, so the token is converted to the concept that it represents
* Examples: dogs become dog, walked becomes walk, 
* lower, but more accurate.Example: caring becomes care, changing becomes change, better becomes good

---

## 2. Data cleaning and pre-processing

More advanced pre-processing tasks (only applied to specific analyses):

* POS or Part-Of-Speech tagging (nouns, verbs, adjectives, etc.) 
* NER or Named Entity Recognition tagging (person, place, company, etc.)
* Parsing

---

## 3. Data transformation

Transformation means *converting the text into numbers*, e.g. some sort of quantifiable measure that a computer can process.

Usually you want to transform your raw textual data (your document) into a vector of countable units:

* Bag-of-words model: creates a document-term matrix (one row for each document, and one column for each term)
* Word embedding models

---

## 4. Perform analysis

This week we learn the following:

* Basic exploratory analysis
    * Word frequency
    * TF-IDF (weighted version of word frequency)
    * Correlations
    
* More Advanced
    * Sentiment analysis
    * Topic modeling

---

class: inverse, middle

# Text analysis with R tidyverse 


---

### The tidy text format

There are different ways to complete all steps in R, and different packages have their own approach. We learn how to perform these operations within the tidyverse. 

For the data cleaning and pre-processing step: we start by converting text into a tidy format, which follows the same principle of tidy analysis we have learned so far.

Take a look at [Figure 1.1](https://www.tidytextmining.com/tidytext.html#fig:tidyflow-ch1) from the assigned readings

---

### The tidy text format

A tidy text format is defined as **a table with one-token-per-row** (this is different from the document-term matrix, which has one-document-per-row and one-term-per-column )

Steps:

* take your text
* put into a tibble
* convert into the tidy text format using `unnest_tokens()` 
  * punctuation is automatically removed
  * lower case is automatically applied

[`tidytext`](https://github.com/juliasilge/tidytext)

---

### The tidy text format

Examples of data cleaning and pre-processing using R tidyverse: 

Example 1: **book 1.2 Emily Dickinson example**

Example 2: **book 1.3 Jane Austen example**

They are both in your in-class materials for today.

---


# Get text corpa

```{r janeausten-corpa, echo = FALSE}
library(janeaustenr)

(books <- austen_books() %>%
    group_by(book) %>%
    mutate(linenumber = row_number(),
           chapter = cumsum(str_detect(text,
                                       regex("^chapter [\\divxlc]",
                                             ignore_case = TRUE)))) %>%
    ungroup())
```

---

# Tokenize text

```{r janeausten-token, dependson = "janeausten-corpa"}
(tidy_books <- books %>%
   unnest_tokens(output = word, input = text))
```

---

# Practice using `tidytext`

> How often is each U.S. state mentioned in a popular song?

* Billboard Year-End Hot 100 (1958-present)
* Census Bureau ACS

---

# Song lyrics

* [Reference](https://youtu.be/OPf0YbXqDm0?t=91)

```{r lyrics-import, include = FALSE}
song_lyrics <- here("static", "data", "billboard_lyrics_1964-2015.csv") %>%
  read_csv()
```

```{r lyrics-example, dependson = "lyrics-import", echo = FALSE}
song_lyrics %>%
  filter(Song == "uptown funk") %$%
  Lyrics %>%
  str_wrap() %>%
  cat()
```

---

<div style="width:100%;height:0;padding-bottom:56%;position:relative;"><iframe src="https://giphy.com/embed/2aJM3TUEaY9Yz166e8" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 10)
```

---

# Sentiment analysis

> I am happy

---

# Dictionaries

```{r bing}
get_sentiments("bing")
```

---

# Dictionaries

```{r afinn}
#get_sentiments("afinn")
```

---

# `janeaustenr`

.pull-left[

##### Sense and Sensibility

```{r echo = FALSE}
include_graphics(path = "https://www.quotemaster.org/images/ff/ff37422dc1a2c38dfa293ed3a0d65aa7.gif")
include_graphics(path = "https://smartbitchestrashybooks.com/WP/wp-content/uploads/2016/07/Hugh-Grant-waving-to-Emma-Thompson.gif")
```

]

.pull-right[

##### Pride and Prejudice

```{r echo = FALSE}
include_graphics(path = "https://media.giphy.com/media/26FL4zFEQlJ2ffxXW/giphy.gif")
include_graphics(path = "https://media.giphy.com/media/l4JyVmADBclbnDieY/giphy.gif")
```

]

---

<div style="width:100%;height:0;padding-bottom:56%;position:relative;"><iframe src="https://giphy.com/embed/2wXrSikk2c8llaunlr" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

---

# Calculate sentiment

```{r janeausten-sentiment, dependson = c("janeausten-corpa", "janeausten-token"), echo = FALSE}
janeaustensentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

# plot the sentiment over time in each book
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
        geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
        facet_wrap( ~ book, ncol = 2, scales = "free_x")
```

---

# Load Harry Potter text

```{r hp, echo = FALSE}
library(harrypotter)

# names of each book
hp_books <- c("philosophers_stone", "chamber_of_secrets",
              "prisoner_of_azkaban", "goblet_of_fire",
              "order_of_the_phoenix", "half_blood_prince",
              "deathly_hallows")

# combine books into a list
hp_words <- list(
  philosophers_stone,
  chamber_of_secrets,
  prisoner_of_azkaban,
  goblet_of_fire,
  order_of_the_phoenix,
  half_blood_prince,
  deathly_hallows
) %>%
  # name each list element
  set_names(hp_books) %>%
  # convert each book to a data frame and merge into a single data frame
  map_df(as_tibble, .id = "book") %>%
  # convert book to a factor
  mutate(book = factor(book, levels = hp_books)) %>%
  # remove empty chapters
  drop_na(value) %>%
  # create a chapter id column
  group_by(book) %>%
  mutate(chapter = row_number(book)) %>%
  # tokenize the data frame
  unnest_tokens(word, value)

hp_words
```

---

# Most frequent words, by book

```{r word-freq, echo = FALSE}
hp_words %>%
  # delete stopwords
  anti_join(stop_words) %>%
  # summarize count per word per book
  count(book, word, sort = TRUE) %>%
  # get top 15 words per book
  group_by(book) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, n, book)) %>%
  # create barplot
  ggplot(aes(x = word, y = n, fill = book)) + 
  geom_col(color = "black") +
  scale_x_reordered() +
  labs(title = "Most frequent words in Harry Potter",
       x = NULL,
       y = "Word count") +
  facet_wrap(~ book, scales = "free", nrow = 2) +
  coord_flip() +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")
```

---

# Exercises

<div style="width:100%;height:0;padding-bottom:59%;position:relative;"><iframe src="https://giphy.com/embed/pI2paNxecnUNW" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 10)
```
