---
title: "Getting data from the web: API access"
author: "MACSS 30500 <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "24px",
  code_font_size = "20px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
```

```{r pkgs, include = FALSE, cache = FALSE}
library(tidyverse)
library(broom)
library(jsonlite)
library(stringr)
library(XML)
library(curl)
library(repurrrsive)
library(listviewer)
library(wordcloud)
library(tidytext)
library(manifestoR)
library(httr)
library(rtweet)
library(viridis)
library(flipbookr)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

class: inverse, middle

# Getting data from the web (aka web scraping)

---

```{r fig.align = "center", echo = FALSE, out.width = "70%"}
knitr::include_graphics(path = "webscraping.png", error = FALSE)
```

Image Source [at this link](https://medium.com/analytics-vidhya/web-scraping-in-python-for-data-analysis-6bf355e4fdc8)

---

## Web scraping

Web scraping is **the process of collecting or "scraping" information from a website**. 

If you have ever copied and pasted information from the Internet, you have performed the same task as any scraper, just on a small scale! Web scraping allows automating this process to collect hundreds, thousands, or even millions of information. 

--

Examples: 
* companies' names, emails, and phone numbers 
* newspaper articles 
* customer reviews
* products' prices, descriptions, and characteristics 
* real estate data
* statistical data
* social media data
* etc.

---

## Two main ways to obtain data from the web

**Option 1: Using a web API** 

  > A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.

<!-- more formal definition later -->

**Option 2: Directly scraping the website**

  > Every website, behind the pretty display we see, is made up of code (a mix of HTML, CSS and Javascript). Therefore, each website, and the information stored in it, are directly accessible by users, if they know how to interact with the website's raw code. 

  > You can always scrape a website directly. Often this is necessary. However, if a website has an API, the general rule is to use it.


---

## Two main ways to obtain data from the web

**Option 1: Using a web API** 

  > A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.
  
When using an API, sometimes we have two approaches available to us:

* approach 1: using an API through an R package that someone has written to interact with that API. The package acts as a **wrapper for the given API**, and is generally easier to use than the API. If such a package exists (not all APIs have a wrapper in R) all you have to do is to install it, and learn how to use it.

* approach 2: if no R wrapper exists, we can **directly using the API** provided by the website. In this case, the user relies on R to interact directly with the website's API. 



---

## Our plan

Today we focus on option 1, using APIs and we see examples of both approaches (with and without a wrapper). Next lecture we focus on option 2, direct scraping. 

--

Before we delve into the APIs world, there are two broad concepts that we need to learn:

> Concept 1: **What is behind a website** (e.g., what websites are made of)

> Concept 2: **How the web works** (e.g., how computers interact on the web)

---

class: inverse, middle

# Concept 1: What is behind a website (e.g., what websites are made of)

---

## What is behind a website

A website is made of the following elements:

+ **HTML**, which means *HyperText Markup Language*, is the core element of a website. HTML uses a set of tags to organize the webpage (i.e., makes the text bold, creates body text, paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden

+ **CSS**, which means *Cascading Style Sheets*, adds styling to make the page looks nicer

+ **Javascript (JS)** code, which is used to add interactive elements to the page (you need "dynamic web scraping" techniques to interact with JS)

+ **Other stuff** such as images (.jpg and .png allow webpages to show pictures), hyperlinks, videos or multimedia

---

## HTML

* most important element we need to know for web scraping
* makes the "skeleton" or structure of a website
* quite messy to read, but it follows a hierarchical, tree-like structure since it embeds tags within tags (everything marked with `<>` is a tag)

Standard HTML syntax, simplified example:
```
   <html>
     <head>
        <title>general info about the page</title>
     </head>
     <body>
       <p>a paragraph with some text about the page</p>
       <p>another paragraph with more text</p>
       <p>...</p>
     </body>
   </html>
   
```

Tree-like structure: [a visual example](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108)


---

## HTML Tags

In web scraping, tags are fundamental because we collect information from websites using them. 

Tags:

* are organized in a tree-like structure and are nested within each other
* go in pairs: one on each end of the content that they display; for example `<p>ciao</p>` only the word "ciao" shows up on the webpage
* can have attributes which provide more information
* list of tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element

<!-- add more info here
https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping
* more frequently used tags
* more on tags attributes
* CSS 
* CSS and HTML
* see staff from my Python course, lecture 2 (e.g. every page is different, no perfect structure,etc.)
-->

--

**Knowing how read the HTML (and CSS) language, is fundamental for web scraping.** Especially when we scrape the website directly. When we use an API and/or an API wrapper, this is less important, but still useful.

---

class: inverse, middle

# Concept 2: How the web works (e.g., how computers interact on the web)

---

## How do computer interact on the web? Theory

Computers talk to each other on the web by sending and receiving GET **data requests** and POST **data responses**: some making requests, some receiving and answering them, some doing both. Every computer has an address that other computers can refer to.

When you click on a webpage, the **web browser** of your computer (e.g., chrome, safari, etc.) makes a data request to the **web server** of that page (a database where all the info about that page are stored), and gets back a response

```{r fig.align = "center", echo = FALSE, out.width = "50%"}
knitr::include_graphics(path = "request_response.png", error = FALSE)
```

Image Source [at this link](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor)

---

## How do computer interact on the web? Example

Navigating the web basically means sending a bunch of GET requests to different servers and asking back different files written in HTML.

If you type `https://macss.uchicago.edu/content/current-student-resources` into your web browser and hit enter, these steps occurs under the hood:

1. your web browser translates what you typed into a HTTP request to tell the `macss uchicago` web server that you would like to access the info stored at `/content/current-student-resources` and that the protocol to use is the `http`

1. the web server that hosts `macss uchicago` receives your request and sends back to your web browser an HTTP response and the response content (a bunch of files written in HTML)

1. your browser received them and transforms them into a nice visual display that might include texts, graphics, hyperlinks, etc.

When we do this with the goal of scraping data, there are packages in R that perform these steps for us, either via APIs or without. 


---

class: inverse, middle

# API: Application Programming Interface

---

## API: Terminology

  > A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.

The majority of web APIs use a particular style know as **REST** or **RESTful** which stays for  *Representational State Transfer*. These allows to query the website database using URLs, just like you would construct an URL to view a web page.

An **URL**, which stays for *Uniform Resource Location*, is a string of characters that uses the **HTTP** or *HyperText Transfer Protocol*, to send request for data.

---

## API: Example

**The process of web browsing described in the `macss uchicago` example, is very similar to database querying via RESTful APIs, with only a few changes**:

* the character string that you use for sending requests to an API (e.g. the URL or website address) will have search terms and/or filtering parameters, and authentication codes specific to that API

* the response you get back from the API server is not formatted as HTML, but it will likely be raw text response

* you need R to parse that response and convert it into a format that you like (dataframe, lists, etc.) and then export it as .csv or .json

---
## Using APIs in R

  > A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.
  
When using an API, sometimes we have two approaches available to us:

* approach 1: using an API through an R package that someone has written to interact with that API. The package acts as a **wrapper for the given API**, and is generally easier to use than the API. If such a package exists (not all APIs have a wrapper in R) all you have to do is to install it, and learn how to use it.

* approach 2: if no R wrapper exists, we can **directly using the API** provided by the website. In this case, the user relies on R to interact directly with the website's API. 

---

## Using APIs with a wrapper package

* Packages with R functions written for existing APIs
* Useful because:
    * Reproducible
    * Up-to-date (ideally)
    * Ease of access

---

## Using APIs with a wrapper package

Examples (in-class):

**Wordbank database**: 
* rich set of socioeconomic indicators spanning several decades and dozens of topics
* full data available for bulk download as CSV files from their website (see HW5); but frequently you only need to obtain a handful of indicators or a subset of countries
* the `wbstats` package provides a wrapper for R for easier access to the API; it returns the results in a tidy data frame.

**GeoNames geographical database**:
* geographical information for all countries and other locations
* the `geonames` package provides a wrapper for R.

---

## Using APIs with a wrapper package

Examples (work through them on your own, script and explanation are provided in the class materials for today):

**Census Bureau**:
* statistical data from the US Census Bureau
* the `tidycensus` provides a wrapper for R for: the US Census Bureau’s Census and five-year American Community Survey APIs.

**Manifesto Project**: 
* political party manifestos from around the world
* covers 1,000+ parties from 1945 until today in 50+ countries on five continents. 
* the `manifestoR` package provides a wrapper for R: https://github.com/ManifestoProject/manifestoR

**Full list of APIs that target political science data**: https://ucsd.libguides.com/c.php?g=90743&p=3202435

---

## Using APIs directly

* No wrapper package 
* Write your own function!
* Example: **OMD Open Movie Database**

---

## Using APIs in R: register for access

**Many APIs require you to register for access.** Sometimes this is as quick as providing email and password (and usually you receive an email with your private API key). Other times you have to submit an application and go through a review process (e.g., Twitter).

Often this process is free, but some APIs require you to pay a few. 

Registering for access, allows APIs to track which users are submitting queries and manage demand. If you submit too many queries too quickly, you might be **rate-limited** and your requests de-prioritized or blocked: when in doubt, check the API access policy of the web site to determine what these limits are.

---

## Using APIs in R: store username and other private info

**To tell R your APIs username (and key, if necessary), you have two options**. For example, the `geonames` API requires you to register and set a username:

1. You could run the line `options(geonamesUsername = "my_user_name")` in R.

1. However, this is insecure, especially if you plan to put your work on GitHub. We don't want to risk committing this line and pushing it to our public GitHub page. Instead, you should (please do so for your HW7):
  * create a file in the same place as your `.Rproj` file. To do that, run the following command from the R console `usethis::edit_r_profile(scope = "project")`. This will create a special file called `.Rprofile` in the same directory as your `.Rproj` file (assuming you are working in an R project)
  * the file should open automatically in your RStudio script editor; otherwise open the file and add `options(geonamesUsername = "my_user_name")` to that file, replacing `my_user_name` with your Geonames username.

---

## Using APIs in R: store username and other private info

Important:
* Make sure your `.Rprofile` ends with a blank line
* Make sure `.Rprofile` is included in your `.gitignore` file, otherwise it will be synced with Github
* Restart RStudio after modifying `.Rprofile` in order to load any new keys into memory
* Spelling is important when you set the option in your `.Rprofile`

---

## Acknowledgments 

APIs explanation and examples are drawn from Rochelle Terman’s "Finding APIs" page [here](https://plsc-31101.github.io/course/collecting-data-from-the-web.html#finding-apis) and Benjamin Soltoff’s “Computing for the Social Sciences” course materials, licensed under the CC BY NC 4.0 Creative Commons License. Any errors or oversights are mine alone.
