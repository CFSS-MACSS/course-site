<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Getting data from the web: API access</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACSS 30500   University of Chicago" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Getting data from the web: API access
]
.author[
### MACSS 30500 <br /> University of Chicago
]

---






class: inverse, middle

# Getting data from the web (aka web scraping)

---

&lt;img src="webscraping.png" width="70%" style="display: block; margin: auto;" /&gt;

Image Source [at this link](https://medium.com/analytics-vidhya/web-scraping-in-python-for-data-analysis-6bf355e4fdc8)

---

## Web scraping

Web scraping is **the process of collecting or "scraping" information from a website**. 

If you have ever copied and pasted information from the Internet, you have performed the same task as any scraper, just on a small scale! Web scraping allows automating this process to collect hundreds, thousands, or even millions of information. 

--

Examples: 
* companies' names, emails, and phone numbers 
* newspaper articles 
* customer reviews
* products' prices, descriptions, and characteristics 
* real estate data
* statistical data
* social media data
* etc.

---

## Two main ways to obtain data from the web

**Option 1: Using a web API** 

  &gt; A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.

&lt;!-- more formal definition later --&gt;

**Option 2: Directly scraping the website**

  &gt; Every website, behind the pretty display we see, is made up of code (a mix of HTML, CSS and Javascript). Therefore, each website, and the information stored in it, are directly accessible by users, if they know how to interact with the website's raw code. 

  &gt; You can always scrape a website directly. Often this is necessary. However, if a website has an API, the general rule is to use it.


---

## Two main ways to obtain data from the web

**Option 1: Using a web API** 

  &gt; A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.
  
When using an API, sometimes we have two approaches available to us:

* approach 1: using an API through an R package that someone has written to interact with that API. The package acts as a **wrapper for the given API**, and is generally easier to use than the API. If such a package exists (not all APIs have a wrapper in R) all you have to do is to install it, and learn how to use it.

* approach 2: if no R wrapper exists, we can **directly using the API** provided by the website. In this case, the user relies on R to interact directly with the website's API. 



---

## Our plan

Today we focus on option 1, using APIs and we see examples of both approaches (with and without a wrapper). Next lecture we focus on option 2, direct scraping. 

--

Before we delve into the APIs world, there are two broad concepts that we need to learn:

&gt; Concept 1: **What is behind a website** (e.g., what websites are made of)

&gt; Concept 2: **How the web works** (e.g., how computers interact on the web)

---

class: inverse, middle

# Concept 1: What is behind a website (e.g., what websites are made of)

---

## What is behind a website

A website is made of the following elements:

+ **HTML**, which means *HyperText Markup Language*, is the core element of a website. HTML uses a set of tags to organize the webpage (i.e., makes the text bold, creates body text, paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden

+ **CSS**, which means *Cascading Style Sheets*, adds styling to make the page looks nicer

+ **Javascript (JS)** code, which is used to add interactive elements to the page (you need "dynamic web scraping" techniques to interact with JS)

+ **Other stuff** such as images (.jpg and .png allow webpages to show pictures), hyperlinks, videos or multimedia

---

## HTML

* most important element we need to know for web scraping
* makes the "skeleton" or structure of a website
* quite messy to read, but it follows a hierarchical, tree-like structure since it embeds tags within tags (everything marked with `&lt;&gt;` is a tag)

Standard HTML syntax, simplified example:
```
   &lt;html&gt;
     &lt;head&gt;
        &lt;title&gt;general info about the page&lt;/title&gt;
     &lt;/head&gt;
     &lt;body&gt;
       &lt;p&gt;a paragraph with some text about the page&lt;/p&gt;
       &lt;p&gt;another paragraph with more text&lt;/p&gt;
       &lt;p&gt;...&lt;/p&gt;
     &lt;/body&gt;
   &lt;/html&gt;
   
```

Tree-like structure: [a visual example](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108)


---

## HTML Tags

In web scraping, tags are fundamental because we collect information from websites using them. 

Tags:

* are organized in a tree-like structure and are nested within each other
* go in pairs: one on each end of the content that they display; for example `&lt;p&gt;ciao&lt;/p&gt;` only the word "ciao" shows up on the webpage
* can have attributes which provide more information
* list of tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element

&lt;!-- add more info here
https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping
* more frequently used tags
* more on tags attributes
* CSS 
* CSS and HTML
* see staff from my Python course, lecture 2 (e.g. every page is different, no perfect structure,etc.)
--&gt;

--

**Knowing how read the HTML (and CSS) language, is fundamental for web scraping.** Especially when we scrape the website directly. When we use an API and/or an API wrapper, this is less important, but still useful.

---

class: inverse, middle

# Concept 2: How the web works (e.g., how computers interact on the web)

---

## How do computer interact on the web? Theory

Computers talk to each other on the web by sending and receiving GET **data requests** and POST **data responses**: some making requests, some receiving and answering them, some doing both. Every computer has an address that other computers can refer to.

When you click on a webpage, the **web browser** of your computer (e.g., chrome, safari, etc.) makes a data request to the **web server** of that page (a database where all the info about that page are stored), and gets back a response

&lt;img src="request_response.png" width="50%" style="display: block; margin: auto;" /&gt;

Image Source [at this link](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor)

---

## How do computer interact on the web? Example

Navigating the web basically means sending a bunch of GET requests to different servers and asking back different files written in HTML.

For example, if you type `https://macss.uchicago.edu/content/current-student-resources` into your web browser and hit enter, these steps occurs under the hood:

1. your web browser translates what you typed into a properly formatted HTTP request to tell the `macss uchicago` web server that you would like to access the info stored at `/content/current-student-resources` and that the format or protocol you use for your request is the `http`
1. the web server that hosts `macss uchicago` receives your request and sends back to your web browser an HTTP response together with the response content as a bunch of files written in HTML 
1. your browser received them and transforms them into a nice visual display that might include texts, graphics, hyperlinks, etc.

When we do this with the goal of scraping data, there are packages in R that perform these steps for us, either via APIs or without. 


---

class: inverse, middle

# API: Application Programming Interface

---

## API: Terminology

  &gt; A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.

The majority of web APIs use a particular style know as **REST** or **RESTful** which stays for  *Representational State Transfer*. These allows to query the website database using URLs, just like you would construct an URL to view a web page.

An **URL**, which stays for *Uniform Resource Location*, is a string of characters that uses the **HTTP** or *HyperText Transfer Protocol*, to send request for data.

---

## API: Example

**The process of web browsing described in the `macss uchicago` example, is very similar to database querying via RESTful APIs, with only a few changes**:

* the character string that you use for sending requests to an API (e.g. the URL or website address) will have search terms and/or filtering parameters, and authentication codes specific to that API

* the response you get back from the API server is not formatted as HTML, but it will likely be raw text response

* you need R to parse that response and convert it into a format that you like (dataframe, lists, etc.) and then export it as .csv or .json

---

## Using APIs in R

  &gt; A web API (Application Programming Interface) is an interface provided by the website that helps users to collect data from that specific website.
  
When using an API, sometimes we have two approaches available to us:

* approach 1: using an API through an R package that someone has written to interact with that API. The package acts as a **wrapper for the given API**, and is generally easier to use than the API. If such a package exists (not all APIs have a wrapper in R) all you have to do is to install it, and learn how to use it.

* approach 2: if no R wrapper exists, we can **directly using the API** provided by the website. In this case, the user relies on R to interact directly with the website's API. 

---

## Using APIs with a wrapper package

* Packages with R functions written for existing APIs
* Useful because:
    * Reproducible
    * Up-to-date (ideally)
    * Ease of access

---

## Using APIs with a wrapper package

We examine the following examples:

**Wordbank database**: 
The `wbstats` implements this API in R to allow for relatively easy access to the API and return the results in a tidy data frame.

The World Bank contains a rich and detailed set of socioeconomic indicators spanning several decades and dozens of topics. Their data is available for bulk download as CSV files from their website; you previously practiced importing and wrangling this data for all countries. However as you noted in that assignment, frequently you only need to obtain a handful of indicators or a subset of countries.

To provide more granular access to this information, the World Bank provides a RESTful API for querying and obtaining a portion of their database programmatically. The wbstats implements this API in R to allow for relatively easy access to the API and return the results in a tidy data frame.

**GeoNames geographical database**: Provides lots of geographical information for all countries and other locations. The `geonames` package provides a wrapper for R.

**The Manifesto Project**: Provides text and other information on political party manifestos from around the world. It currently covers over 1,000 parties from 1945 until today in over 50 countries on five continents. The `manifestoR` package provides a wrapper for R.

**The Census Bureau**: Provides datasets from the US Census Bureau. The `tidycensus` provides a wrapper for R // package allows users to interface with the US Census Bureau’s decennial Census and five-year American Community APIs.

A full list of APIs that target political science data can be found here: https://ucsd.libguides.com/c.php?g=90743&amp;p=3202435

---

## Using APIs directly




---

## Acknowledgments 

APIs explanation and examples are drawn from Rochelle Terman’s "Finding APIs" page [here](https://plsc-31101.github.io/course/collecting-data-from-the-web.html#finding-apis) and Benjamin Soltoff’s “Computing for the Social Sciences” course materials, licensed under the CC BY NC 4.0 Creative Commons License. Any errors or oversights are mine alone.

Example taken and developed from Rochelle Terman's website 
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLines": true,
"highlightLanguage": "r",
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
