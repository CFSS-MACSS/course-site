---
title: "Text analysis:  <br /> topic modeling and sentiment analysis"
author: "MACSS 30500 <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#011f4b",
  secondary_color = "#bbd6c7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "24px",
  code_font_size = "20px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
xaringanExtra::use_panelset()
```

```{r pkgs, include = FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE
)

library(tidyverse)
library(tidymodels)
library(tidytext)
library(themis)
library(rjson)
library(topicmodels)
library(here)
library(patchwork)
library(tictoc)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```
# Agenda
* Topic Modeling
  * LDA
* Sentiment Analysis
  * Positive / negative sentiment
* Application: TS!
* Future applications: 
    *author prediction

---
class: inverse, middle

# Topic Modeling

---

# Topic Modeling

Method to organize and understand large collections of textual information (without reading them).

How? 
--
By **finding groups of words ("topics") that go together.** Words that co-occur more frequently that chance alone would predict are assigned to the same topic.

Example: organizing 1.2M books into 2000 topics see David Mimno [slides here](https://mimno.infosci.cornell.edu/slides/details.pdf) and [video here](https://vimeo.com/53080123). 

---

# Topic Modeling

**Word Frequency** looks at the exact recurrence of each word in a corpus (using a simple count or a weighted count).

Instead: 

**Topic Modeling** looks at how groups of words co-occur together (using probability).

--

Intuition: 

The meaning of words is dependent upon the broader context in which they are used. 

---

# Topic Modeling

A topic is a set of words that are associated with one another, showing an underlying common theme that is semantically interpretable by humans. 

A topic is similar to what a human would call a theme or a discourse: whenever a specific discourse is made, some words tend to come up more frequently than others. The goal of TM is to identify the discourses that characterize a collection of documents. 

---

# Topic Modeling

One way to think about how TM…

Imagine working through an article with a set of highlighters. As you read through the article:
  * you use a different color for the key words of themes within the article as you encounter them
  * when you were done, you could take all words of the same color and create separate lists (one per each topic)


---

# Topic Modeling

```{r fig.align = "center", echo = FALSE, out.width = "90%"}
knitr::include_graphics(path = "tm.png", error = FALSE)
```

---

# Topic Modeling: LDA

TM is the name of a family of algorithms. The most common is **LDA or Latent Dirichlet Allocation**. This model assumes that:

1. every document in a corpus contains a mixture of topics that are found throughout the entire corpus

1. each topic is made of a (limited) mixture of characteristic words, which tend to occur together whenever the topic is displayed

The topic structure is hidden ("latent"): we can only observe the documents and words, not the topics themselves; the goal is to infer such topics by the words and documents (more formally: topic modeling computes the conditional posterior distribution of latent variables, e.g. topics, given the observed variables, e.g. words in documents).

---

# Topic Modeling: LDA

**LDA works as follows:**

1. The researcher begins by setting an arbitrary number of topics (`k`) for the whole collection of documents.

1. The algorithm (Gibbs sampling) creates, simultaneously:
    * a first randomly chosen word-topic probability: distribution of topics over all observed words in the collection
    * a first randomly chosen document-topic probability: distribution of topics by document 

1. Through reiterative attempts, the algorithm adjusts these initial distributions to provide a set of probabilities for every word-topic pair and for every document-topic pair. 

---

# Topic Modeling: LDA

**LDA produces two types of output:**

1. words most frequently associated with each of the `k` topics specified by the researcher 

1. documents most frequently associated with each of the `k` topics (the researcher defines a probabilty theresold)

---

# Topic Modeling: LDA simple example

We start with a simple example, then move to a real example in R. Imagine we have a corpus with the following five documents (each document is one sentence):

**Document 1**  I ate a banana and spinach smoothie for breakfast.

**Document 2**  I like to eat broccoli and bananas.

**Document 3**  Puppies and kittens are cute.

**Document 4**  My sister adopted a kitten yesterday.

**Document 5**  This cute hamster is eating a piece of broccoli.

---

# Topic Modeling: LDA simple example

**First, we need to transform these textual data in the appropriate form needed for LDA:**
* input:
  * raw data
* output:
  * create a vocabulary (remove stop words, lowercase, tokenize, etc.)
  * create a document-term matrix

**Then, we run LDA:**
* input:
  * the document-term matrix as input
  * the number of topics we want to generate (we decide them)
* output:
  * the word-topic and document-topic probabilities

---

# Topic Modeling: LDA simple example

If we give to LDA the document-term matrix from these 5 documents, and we ask for 2 topics, LDA might produce something like: 

**Topic A:** 30% eat, 20% broccoli, 15% bananas, 10% breakfast, …

**Topic B:** 20% dog, 20% kitten, 20% cute, 15% hamster, …

--

**Document 1 and 2** 100% Topic A (we can label it "food")

**Document 3 and 4**: 100% Topic B (we can label it "cute animals")

**Document 5:** 60% Topic A, 40% Topic B

---

# Topic Modeling: LDA example in R

Download today's class materials to access the code, we use the textbook example in Chapter 6

**Examples from the book**: Chapter 6, and the three case-study chapters + Chapter 2 to reshape the data into the appropriate format for LDA

---

# Topic modeling: pros/cons

* Topic models **account for "multiplexity":** a given document is unlikely to fall precisely and fully into a single topic 

* Topic models **do not care about the order of words** “dog eats food" is the same as "food eats dog" 

* To determine the "right" number of topics: no fixed rule, it is a try-error process, ultimately **the researcher decides** (there are some metrics, such as perplexity and coherence score but are beyond our goals)

* Topics are **unlabeled:** they are just a bunch of words, it is up to the researcher to read, interpret, and label them

* Topic models are **no substitute for human interpretation of a text:** they are a way of making educated guesses about how words cohere into different latent themes by identifying patterns in the way they co-occur within documents. Many people are somewhat disappointed when they discover their model produces uninformative results

---

# LDAvis

LDAvis is a an interactive visualization of LDA model results

https://github.com/cpsievert/LDAvis


1. What is the meaning of each topic?
1. How prevalent is each topic?
1. How do the topics relate to each other?

---

class: inverse, middle

# Sentiment Analysis

---

# Sentiment Analysis

Use of NLP and programming to **study emotional states and subjective information in a text**. 

In practice sentiment analysis is usually applied:
* to determine the polarity of a text: whether a text is positive, negative, or neutral
* to detect specific feelings and emotions: angry, sad, happy, etc.
* examples: analyzing costumer feedback, classify movies review

Different from Topic Modeling, Sentiment Analysis it is usually supervised (e.g., needs a labeled input to compare our text to)

---

# Sentiment Analysis: our approach

Our approach to sentiment analysis (consistent with the book, Chapter 2): 

* consider the text a combination of its individual words, and 
* consider the sentiment content of the text as the sum of the sentiment content of the individual words

Notice: "This isn’t the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem."

---

# The `sentiments` datasets 

The tidytext package provides access to several sentiment lexicons:

+ **AFINN** from Finn Årup Nielsen (words classified with on a scale from -5 to +5)
+ **bing** from Bing Liu and collaborators (words classified into binary categories: negative and positive)
+ **nrc** from Saif Mohammad and Peter Turney (words classified in multiple categories)

See: https://www.tidytextmining.com/sentiment.html#the-sentiments-datasets

---

# The `sentiments` datasets

These dictionaries constitute our gold standard (e.g., our labeled sentiments): we use them to classify our texts.

Dictionary-based methods like these find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text.

---

# Sentiment Analysis: Examples

**Examples from the book**: Chapter 2, and the three case-study chapters

**Another example (in today's class-materials)**: sentiment analysis of Harry Potter textual data. 

